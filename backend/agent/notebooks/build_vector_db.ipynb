{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17112f47",
   "metadata": {},
   "source": [
    "# Build de Vector DB (Chroma) — Colab + local\n",
    "\n",
    "> Objetivo: construir un artefacto Chroma persistente en disco para búsqueda semántica.\n",
    "\n",
    "**Diseño:** este notebook se puede correr tanto en **Google Colab** (para datasets grandes / GPU) como en **local**.\n",
    "\n",
    "**Output (en este repo):**\n",
    "- `backend/agent/vector_db/` (persist_directory)\n",
    "- `backend/agent/vector_db/manifest.json`\n",
    "- (opcional) `backend/agent/vector_db.zip`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a807f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0) Environment check (Colab/local)\n",
    "import sys, platform\n",
    "print('Python:', sys.version)\n",
    "print('Platform:', platform.platform())\n",
    "\n",
    "# Best-effort GPU check (sentence-transformers usa torch por debajo)\n",
    "try:\n",
    "    import torch\n",
    "\n",
    "    print('torch:', torch.__version__)\n",
    "    print('CUDA available:', torch.cuda.is_available())\n",
    "    if torch.cuda.is_available():\n",
    "        print('GPU:', torch.cuda.get_device_name(0))\n",
    "except Exception as e:\n",
    "    print('GPU check skipped:', repr(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d674c2",
   "metadata": {},
   "source": [
    "## 1) Setup (opcional) — clonar repo (Colab)\n",
    "\n",
    "Este notebook funciona en:\n",
    "- **Local** (VS Code / Jupyter): abre el notebook dentro del repo.\n",
    "- **Google Colab**: activa `DO_CLONE_REPO=True` y configura `REPO_URL`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841d3c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2A) (Opcional) Clonar repo (pensado para Colab)\n",
    "# Si ya estás en tu repo local, deja DO_CLONE_REPO=False.\n",
    "\n",
    "import os\n",
    "\n",
    "DO_CLONE_REPO = False\n",
    "REPO_URL = 'https://github.com/<org-or-user>/<repo>.git'  # <-- cambia esto\n",
    "REPO_DIR = 'Libreria-Aurora'  # carpeta destino (ajusta si necesitas)\n",
    "\n",
    "if DO_CLONE_REPO:\n",
    "    import subprocess\n",
    "    from pathlib import Path\n",
    "\n",
    "    if not Path(REPO_DIR).exists():\n",
    "        print('Cloning:', REPO_URL)\n",
    "        subprocess.run(['git', 'clone', REPO_URL, REPO_DIR], check=True)\n",
    "    else:\n",
    "        print('Repo dir already exists:', REPO_DIR)\n",
    "\n",
    "    os.chdir(REPO_DIR)\n",
    "    print('cwd ->', Path.cwd())\n",
    "else:\n",
    "    print('DO_CLONE_REPO=False (skip)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1ccd25",
   "metadata": {},
   "source": [
    "## 2) Setup (opcional) — instalar dependencias\n",
    "\n",
    "Si corres esto en Colab o en un entorno vacío, puedes activar `DO_INSTALL_DEPS=True` para instalar lo necesario desde `backend/requirements.txt`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80948a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2B) (Opcional) Instalar dependencias (Colab/local)\n",
    "# - En VS Code normalmente ya tienes tu venv y esto no es necesario.\n",
    "# - En Colab sí conviene correrlo una vez.\n",
    "\n",
    "DO_INSTALL_DEPS = False  # <-- cambia a True si estás en Colab o quieres bootstrap automático\n",
    "\n",
    "if DO_INSTALL_DEPS:\n",
    "    import subprocess, sys\n",
    "\n",
    "    def run(cmd) -> None:\n",
    "        print('+', ' '.join(cmd))\n",
    "        subprocess.run(cmd, check=True)\n",
    "\n",
    "    run([sys.executable, '-m', 'pip', 'install', '--upgrade', 'pip'])\n",
    "    run([sys.executable, '-m', 'pip', 'install', '-r', 'backend/requirements.txt'])\n",
    "else:\n",
    "    print('DO_INSTALL_DEPS=False (skip)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6e700e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import hashlib\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "print('cwd:', Path.cwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2b2f95",
   "metadata": {},
   "source": [
    "## 3) Configuración de rutas y parámetros\n",
    "\n",
    "Ajusta si tu `cwd` no es la raíz del repo. Por defecto intenta detectar la raíz buscando `docker-compose.yml` o la carpeta `backend/`.\n",
    "\n",
    "Este notebook es genérico: puedes cambiar el dataset y el output directory sin tocar el resto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f8afe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    cur = start.resolve()\n",
    "    for parent in [cur] + list(cur.parents):\n",
    "        if (parent / 'docker-compose.yml').exists() and (parent / 'backend').exists():\n",
    "            return parent\n",
    "        if (parent / 'backend').exists() and (parent / 'docs').exists():\n",
    "            return parent\n",
    "    return cur\n",
    "\n",
    "\n",
    "REPO_ROOT = find_repo_root(Path.cwd())\n",
    "BACKEND_ROOT = REPO_ROOT / 'backend'\n",
    "\n",
    "# Dataset\n",
    "# - Default: fixture del repo\n",
    "# - Genérico: puedes pasar un JSON alternativo vía env var\n",
    "DEFAULT_DATASET_PATH = BACKEND_ROOT / 'apps' / 'libros' / 'fixtures' / 'libros_prueba.json'\n",
    "DATASET_PATH = Path(os.getenv('VECTOR_DATASET_PATH', str(DEFAULT_DATASET_PATH)))\n",
    "\n",
    "# Output (centralizado en backend/agent/)\n",
    "VECTOR_DB_DIR = BACKEND_ROOT / 'agent' / 'vector_db'\n",
    "MANIFEST_PATH = VECTOR_DB_DIR / 'manifest.json'\n",
    "ZIP_PATH = BACKEND_ROOT / 'agent' / 'vector_db.zip'\n",
    "\n",
    "# Puedes sobreescribir el nombre de colección desde env (alineado al plan)\n",
    "VECTOR_COLLECTION = os.getenv('VECTOR_COLLECTION', 'book_catalog')\n",
    "\n",
    "# Embeddings (default recomendado; puedes cambiarlo)\n",
    "EMBEDDINGS_MODEL = os.getenv('VECTOR_EMBEDDINGS_MODEL', 'mixedbread-ai/mxbai-embed-large-v1')\n",
    "NORMALIZE_EMBEDDINGS = True\n",
    "\n",
    "# Performance knobs (útil para datasets grandes)\n",
    "BATCH_SIZE = int(os.getenv('VECTOR_EMBEDDINGS_BATCH_SIZE', '64'))\n",
    "\n",
    "# Device selection: usa GPU si torch + CUDA están disponibles\n",
    "DEVICE = 'cpu'\n",
    "try:\n",
    "    import torch\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        DEVICE = 'cuda'\n",
    "except Exception:\n",
    "    DEVICE = 'cpu'\n",
    "\n",
    "# Si existe el artefacto, bórralo y reconstruye\n",
    "REBUILD = True\n",
    "\n",
    "print('REPO_ROOT:', REPO_ROOT)\n",
    "print('DATASET_PATH:', DATASET_PATH)\n",
    "print('VECTOR_DB_DIR:', VECTOR_DB_DIR)\n",
    "print('MANIFEST_PATH:', MANIFEST_PATH)\n",
    "print('ZIP_PATH:', ZIP_PATH)\n",
    "print('VECTOR_COLLECTION:', VECTOR_COLLECTION)\n",
    "print('EMBEDDINGS_MODEL:', EMBEDDINGS_MODEL)\n",
    "print('DEVICE:', DEVICE)\n",
    "print('BATCH_SIZE:', BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8d2eb7",
   "metadata": {},
   "source": [
    "## 4) Cargar dataset y construir documentos\n",
    "\n",
    "Este notebook soporta 2 formatos de dataset:\n",
    "\n",
    "1) **Fixture Django** (default en este repo): `backend/apps/libros/fixtures/libros_prueba.json`\n",
    "\n",
    "2) **JSON genérico**: una lista de objetos con al menos `text` (o `page_content`) y opcional `id` y `metadata`.\n",
    "\n",
    "Ejemplo JSON genérico:\n",
    "```json\n",
    "[\n",
    "  {\"id\": \"doc:1\", \"text\": \"Título: ...\\nDescripción: ...\", \"metadata\": {\"source\": \"catalog\"}},\n",
    "  {\"id\": \"doc:2\", \"text\": \"...\", \"metadata\": {\"category\": \"Ficción\"}}\n",
    "]\n",
    "```\n",
    "\n",
    "Para usar tu propio dataset:\n",
    "- Exporta un JSON en uno de esos formatos.\n",
    "- Define `VECTOR_DATASET_PATH` (env var) o edita `DATASET_PATH` en la sección de configuración.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8448479",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "if not DATASET_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f'No se encontró DATASET_PATH={DATASET_PATH}. '\n",
    "        'Ajusta VECTOR_DATASET_PATH (env var) o cambia DATASET_PATH en la sección de configuración.'\n",
    "    )\n",
    "\n",
    "raw = json.loads(DATASET_PATH.read_text(encoding='utf-8'))\n",
    "\n",
    "# Soporta 2 formatos:\n",
    "# A) Fixture Django (lista de objetos con model/pk/fields)\n",
    "# B) JSON genérico (lista de dicts con al menos \"text\" y opcional \"id\"/\"metadata\")\n",
    "\n",
    "def is_django_fixture(items) -> bool:\n",
    "    if not isinstance(items, list) or not items:\n",
    "        return False\n",
    "    first = items[0]\n",
    "    return isinstance(first, dict) and 'model' in first and 'pk' in first and 'fields' in first\n",
    "\n",
    "\n",
    "docs: list[Document] = []\n",
    "ids: list[str] = []\n",
    "dataset_format = 'unknown'\n",
    "counts: dict[str, int] = {}\n",
    "\n",
    "if is_django_fixture(raw):\n",
    "    dataset_format = 'django_fixture'\n",
    "\n",
    "    categorias: dict[int, dict[str, Any]] = {}\n",
    "    libros: list[dict[str, Any]] = []\n",
    "\n",
    "    for item in raw:\n",
    "        model = item.get('model')\n",
    "        pk = item.get('pk')\n",
    "        fields = item.get('fields', {})\n",
    "        if model == 'libros.categoria':\n",
    "            categorias[int(pk)] = fields\n",
    "        elif model == 'libros.libro':\n",
    "            libros.append({'pk': int(pk), **fields})\n",
    "\n",
    "    def libro_to_text(libro: dict[str, Any]) -> str:\n",
    "        categoria_nombre = categorias.get(int(libro.get('categoria') or 0), {}).get('nombre', '')\n",
    "        parts = [\n",
    "            f\"Título: {libro.get('titulo','')}\",\n",
    "            f\"Autor: {libro.get('autor','')}\",\n",
    "            f\"Categoría: {categoria_nombre}\" if categoria_nombre else None,\n",
    "            f\"Editorial: {libro.get('editorial','')}\" if libro.get('editorial') else None,\n",
    "            f\"Año: {libro.get('año_publicacion','')}\" if libro.get('año_publicacion') else None,\n",
    "            f\"ISBN: {libro.get('isbn','')}\" if libro.get('isbn') else None,\n",
    "            f\"Descripción: {libro.get('descripcion','')}\" if libro.get('descripcion') else None,\n",
    "            f\"Precio: {libro.get('precio','')}\" if libro.get('precio') else None,\n",
    "        ]\n",
    "        return '\\n'.join([p for p in parts if p])\n",
    "\n",
    "    for libro in libros:\n",
    "        categoria_id = int(libro.get('categoria') or 0)\n",
    "        categoria_nombre = categorias.get(categoria_id, {}).get('nombre')\n",
    "        metadata = {\n",
    "            'libro_id': libro.get('pk'),\n",
    "            'titulo': libro.get('titulo'),\n",
    "            'autor': libro.get('autor'),\n",
    "            'isbn': libro.get('isbn'),\n",
    "            'categoria_id': categoria_id or None,\n",
    "            'categoria': categoria_nombre,\n",
    "            'editorial': libro.get('editorial'),\n",
    "            'anio_publicacion': libro.get('año_publicacion'),\n",
    "            'precio': libro.get('precio'),\n",
    "            'stock': libro.get('stock'),\n",
    "        }\n",
    "        docs.append(Document(page_content=libro_to_text(libro), metadata=metadata))\n",
    "        ids.append(f\"libro:{libro.get('pk')}\")\n",
    "\n",
    "    counts = {\n",
    "        'categorias': len(categorias),\n",
    "        'libros': len(libros),\n",
    "        'documents_indexed': len(docs),\n",
    "    }\n",
    "\n",
    "else:\n",
    "    dataset_format = 'generic_json'\n",
    "\n",
    "    if not isinstance(raw, list):\n",
    "        raise ValueError('Formato no soportado: se esperaba una lista JSON.')\n",
    "\n",
    "    for i, item in enumerate(raw):\n",
    "        if not isinstance(item, dict):\n",
    "            continue\n",
    "\n",
    "        text = item.get('text') or item.get('page_content')\n",
    "        if not text:\n",
    "            continue\n",
    "\n",
    "        metadata = item.get('metadata') or {}\n",
    "        if not isinstance(metadata, dict):\n",
    "            metadata = {'metadata': str(metadata)}\n",
    "\n",
    "        doc_id = item.get('id') or item.get('doc_id') or f'doc:{i}'\n",
    "        docs.append(Document(page_content=str(text), metadata=metadata))\n",
    "        ids.append(str(doc_id))\n",
    "\n",
    "    counts = {\n",
    "        'records': len(raw),\n",
    "        'documents_indexed': len(docs),\n",
    "    }\n",
    "\n",
    "print('dataset_format:', dataset_format)\n",
    "print('docs:', len(docs))\n",
    "print('sample doc:', (docs[0].page_content[:200] if docs else '<none>'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3049bd",
   "metadata": {},
   "source": [
    "## 5) Construir Chroma (persistente en disco)\n",
    "\n",
    "Esto creará/actualizará la colección en `VECTOR_DB_DIR`.\n",
    "\n",
    "Para datasets grandes, lo normal es correr esto en Colab (GPU) y luego bajar `vector_db.zip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6d9ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies for vector DB and embeddings\n",
    "# Colab Scenario\n",
    "# Temporarily install missing package if not in requirements.txt (ideally add to requirements.txt)\n",
    "!pip install langchain-chroma\n",
    "!pip install langchain-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cf7379",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "if REBUILD and VECTOR_DB_DIR.exists():\n",
    "    shutil.rmtree(VECTOR_DB_DIR)\n",
    "\n",
    "VECTOR_DB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDINGS_MODEL,\n",
    "    model_kwargs={'device': DEVICE},\n",
    "    encode_kwargs={\n",
    "        'normalize_embeddings': NORMALIZE_EMBEDDINGS,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "    },\n",
    ")\n",
    "\n",
    "store = Chroma(\n",
    "    collection_name=VECTOR_COLLECTION,\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=str(VECTOR_DB_DIR),\n",
    ")\n",
    "\n",
    "# Indexar (IDs estables por libro)\n",
    "store.add_documents(documents=docs, ids=ids)\n",
    "\n",
    "print('Indexado OK. Persist dir:', VECTOR_DB_DIR)\n",
    "print('Colección:', VECTOR_COLLECTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac83109",
   "metadata": {},
   "source": [
    "## 6) Validación rápida (smoke test)\n",
    "\n",
    "Ejecuta una búsqueda simple contra la DB para confirmar que el índice funciona y devuelve resultados plausibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6201b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Cien años de soledad'\n",
    "hits = store.similarity_search(query, k=3)\n",
    "\n",
    "print('query:', query)\n",
    "for i, d in enumerate(hits, start=1):\n",
    "    md = d.metadata\n",
    "    print(f\"#{i} libro_id={md.get('libro_id')} | titulo={md.get('titulo')} | autor={md.get('autor')} | categoria={md.get('categoria')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d0bce7",
   "metadata": {},
   "source": [
    "## 7) Escribir `manifest.json`\n",
    "\n",
    "El manifest te ayuda a asegurar reproducibilidad:\n",
    "- qué embeddings/modelo se usó\n",
    "- cuándo se construyó\n",
    "- con qué dataset (hash)\n",
    "\n",
    "Esto es clave cuando simulas un catálogo grande y haces rebuilds en Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f69a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sha256_file(path: Path) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with path.open('rb') as f:\n",
    "        for chunk in iter(lambda: f.read(1024 * 1024), b''):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "\n",
    "dataset_sha256 = sha256_file(DATASET_PATH)\n",
    "built_at = datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "manifest = {\n",
    "    'built_at_utc': built_at,\n",
    "    'dataset': {\n",
    "        'path': str(DATASET_PATH),\n",
    "        'sha256': dataset_sha256,\n",
    "        'format': dataset_format,\n",
    "    },\n",
    "    'vector_db': {\n",
    "        'provider': 'chroma',\n",
    "        'persist_directory': str(VECTOR_DB_DIR),\n",
    "        'collection': VECTOR_COLLECTION,\n",
    "    },\n",
    "    'embeddings': {\n",
    "        'model_name': EMBEDDINGS_MODEL,\n",
    "        'normalize_embeddings': bool(NORMALIZE_EMBEDDINGS),\n",
    "        'device': DEVICE,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "    },\n",
    "    'document_schema': {\n",
    "        'notes': 'page_content es el texto embeddeado; metadata son campos de salida/filtros.',\n",
    "    },\n",
    "    'counts': counts,\n",
    "}\n",
    "\n",
    "MANIFEST_PATH.write_text(json.dumps(manifest, ensure_ascii=False, indent=2), encoding='utf-8')\n",
    "print('Manifest escrito en:', MANIFEST_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900e0fa5",
   "metadata": {},
   "source": [
    "## 8) Exportar `vector_db/` como ZIP (para llevarlo a otra máquina)\n",
    "\n",
    "Esto es lo más útil cuando corres el build en Colab: te llevas el artefacto ya embeddeado a tu máquina local.\n",
    "\n",
    "- Output: `backend/agent/vector_db.zip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69bd82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Asegura que exista el artefacto\n",
    "if not VECTOR_DB_DIR.exists():\n",
    "    raise FileNotFoundError(f'No existe VECTOR_DB_DIR: {VECTOR_DB_DIR} (¿corriste el build?)')\n",
    "\n",
    "# No generamos el manifest aquí: solo lo exportamos.\n",
    "# Si no existe, falla para que corras el paso de manifest primero.\n",
    "if not MANIFEST_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f'No existe MANIFEST_PATH: {MANIFEST_PATH}. '\n",
    "        'Corre primero el paso \"Escribir manifest.json\" y luego vuelve a exportar.'\n",
    "    )\n",
    "\n",
    "# Zippea la carpeta vector_db/ completa, preservando el directorio (vector_db/...) dentro del zip.\n",
    "# Esto evita que al descomprimir quede todo \"suelto\" en la raíz.\n",
    "if ZIP_PATH.exists():\n",
    "    ZIP_PATH.unlink()\n",
    "\n",
    "archive_base = str(ZIP_PATH.with_suffix(''))  # shutil.make_archive agrega .zip\n",
    "shutil.make_archive(\n",
    "    archive_base,\n",
    "    'zip',\n",
    "    root_dir=str(VECTOR_DB_DIR.parent),\n",
    "    base_dir=str(VECTOR_DB_DIR.name),\n",
    ")\n",
    "\n",
    "print('ZIP creado:', ZIP_PATH)\n",
    "print('Incluye manifest:', MANIFEST_PATH.name)\n",
    "print('Tamaño (bytes):', ZIP_PATH.stat().st_size)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
